{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "congressional-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "simple-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autograd\n",
    " # torc.autograd is an automatic differentiation tool for use in NN training, backprop with gradient    descent, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "magnetic-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstration of a single training step\n",
    "model = torchvision.models.resnet18(pretrained=True)  # pretrained model provided\n",
    "data = torch.rand(1,3,64,64)  # 1 image, 3 channels, 64x64\n",
    "labels = torch.rand(1,1000)   # Random initial labels\n",
    "\n",
    "# forward pass\n",
    "pred = model(data)\n",
    "\n",
    "# Backward Pass\n",
    "loss = (pred-labels).sum()  #tensor(-520.9473, grad_fn=<SumBackward0>)\n",
    "# The loss calculation defines the gradient function. Autograd will calculate the gradients and store them in the model.{parameter}grad attribute\n",
    "loss.backward()\n",
    "\n",
    "# Optimization\n",
    "# We can create an optimizer, and specify the optimization approach. Example uses Stochastic Gradient Descent. Learning Rate = .01 and momentum - 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optimizer.step()  # This adjusts each parameter by the gradient stored in .grad affter .backwards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dental-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How Autograd works under the hood\n",
    "a = torch.tensor([2.,3.], requires_grad=True)  # require_grad will track every op on the tensor\n",
    "b = torch.tensor([6.,4.], requires_grad=True)\n",
    "\n",
    "## Imagine a and b are neural network parameters, let's define the error as Q = 3a^3 - b^2\n",
    "# So, the gradient with respect to a: dQ/da = 9a^2\n",
    "# the gradient with respect to b: dQ/db = -2b\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "## We can all .backward() on Q, and the above gradients will be calculated.\n",
    "# We need to pass a gradient argument equivalent to backwards, which == torch.ones(Q.shape)\n",
    "Q.backward(gradient=torch.ones(Q.shape))\n",
    "\n",
    "# Let's check of our math is right\n",
    "assert all(a.grad == 9*a.mul(a))  # 9a^2\n",
    "assert all(b.grad == -2*b)        # -2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "honest-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Calculus using Autograd\n",
    "# let y> = f(x>) where y>, x> indicate that y and x are vectors\n",
    "\n",
    "# The gradient of y> with respect to x> is a jacobian Matrix: (dy/dx1, dy/dx2...dy/dxn). Assuming Y is an m dimensional vecotr, it follows that J = (dy1/dx1 ... dy1.dxn\n",
    "#                    .        .      .            \n",
    "#                    dym/dx1 ... dym/dxn )\n",
    "#\n",
    "# \"Generally speaking\", torch.autograd is a Ti84 that can compute a vector-Jacobian product. i.e. give a vector v>, compute\n",
    "# J.T * v\n",
    "#\n",
    "# If v is a gradient of scalar function: l = g(y>) --> v = (dl/dy1 ... dldym).T\n",
    "# THEN (chain rule), the vector-Jacobian product J.T * v IS THE GRADIENT OF l WITH RESPECT TO x\n",
    "# \n",
    "# J.T * v = [[dy1/dx1...dym.dx1] ... [dy1/dxn...dym/dxn]] * [dl/dy1 ... dl/dym] == [dl/dx1...dl/dxn]\n",
    "# ***v> is what torch.ones(Q.shape) is above***\n",
    "#\n",
    "# !! Okay so v> = [1,1]. So [1,1] is the gradient of some scalar function l = g(y>).  I think Q> == y> in this case. We define\n",
    "# !! g as some function on Q> that produces a scalar. If g = sum(), then the gradient of l=sum(y>), with Q> having two \n",
    "# !! dimensions, is like taking the gradient of l=sum([a,b]), then we have [dl/da, dl/db] == [1,1].\n",
    "# !!\n",
    "# TODO: !DF! re-review this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sixth-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Come back and read https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
