{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simple-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autograd\n",
    " # torc.autograd is an automatic differentiation tool for use in NN training, backprop with gradient    descent, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "magnetic-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstration of a single training step\n",
    "model = torchvision.models.resnet18(pretrained=True)  # pretrained model provided\n",
    "data = torch.rand(1,3,64,64)  # 1 image, 3 channels, 64x64\n",
    "labels = torch.rand(1,1000)   # Random initial labels\n",
    "\n",
    "# forward pass\n",
    "pred = model(data)\n",
    "\n",
    "# Backward Pass\n",
    "loss = (pred-labels).sum()  #tensor(-520.9473, grad_fn=<SumBackward0>)\n",
    "# The loss calculation defines the gradient function. Autograd will calculate the gradients and store them in the model.{parameter}grad attribute\n",
    "loss.backward()\n",
    "\n",
    "# Optimization\n",
    "# We can create an optimizer, and specify the optimization approach. Example uses Stochastic Gradient Descent. Learning Rate = .01 and momentum - 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optimizer.step()  # This adjusts each parameter by the gradient stored in .grad affter .backwards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dental-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How Autograd works under the hood\n",
    "a = torch.tensor([2.,3.], requires_grad=True)  # require_grad will track every op on the tensor\n",
    "b = torch.tensor([6.,4.], requires_grad=True)\n",
    "\n",
    "## Imagine a and b are neural network parameters, let's define the error as Q = 3a^3 - b^2\n",
    "# So, the gradient with respect to a: dQ/da = 9a^2\n",
    "# the gradient with respect to b: dQ/db = -2b\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "## We can all .backward() on Q, and the above gradients will be calculated.\n",
    "# We need to pass a gradient argument equivalent to backwards, which == torch.ones(Q.shape)\n",
    "Q.backward(gradient=torch.ones(Q.shape))\n",
    "\n",
    "# Let's check of our math is right\n",
    "assert all(a.grad == 9*a.mul(a))  # 9a^2\n",
    "assert all(b.grad == -2*b)        # -2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "honest-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Calculus using Autograd\n",
    "# let y> = f(x>) where y>, x> indicate that y and x are vectors\n",
    "\n",
    "# The gradient of y> with respect to x> is a jacobian Matrix: (dy/dx1, dy/dx2...dy/dxn). Assuming Y is an m dimensional vecotr, it follows that J = (dy1/dx1 ... dy1.dxn\n",
    "#                    .        .      .            \n",
    "#                    dym/dx1 ... dym/dxn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "quarterly-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "conceptual-publication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-passion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
